import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
from scipy.stats import spearmanr, shapiro, levene, f_oneway, kruskal, kstest
from sklearn.decomposition import PCA
from sklearn.experimental import enable_iterative_imputer  # N√©cessaire pour activer IterativeImputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.linear_model import BayesianRidge
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

def test_shapiro_safe(data, max_sample_size=5000):
    """Test de Shapiro-Wilk sur un √©chantillon limit√© √† max_sample_size pour √©viter l'erreur de taille."""
    sample = data.sample(n=min(len(data), max_sample_size), random_state=42)
    return shapiro(sample)


def plot_pca_kmeans_projection(df, variables_pc_x, variables_pc_y, n_clusters=3, pc_x=0, pc_y=1):
    """
    Fonction pour r√©aliser la projection des individus sur un plan factoriel d√©fini par PCX et PCY,
    en appliquant une ACP (PCA) sur un ensemble de variables et un clustering K-means.

    :param df: DataFrame pandas contenant les donn√©es brutes.
    :param variables_pc_x: Liste des variables utilis√©es pour l'axe X dans la PCA.
    :param variables_pc_y: Liste des variables utilis√©es pour l'axe Y dans la PCA.
    :param n_clusters: Nombre de clusters pour le K-means (par d√©faut 3).
    :param pc_x: Index de la composante principale √† afficher sur l'axe X (ex: 0 pour PC1, 2 pour PC3).
    :param pc_y: Index de la composante principale √† afficher sur l'axe Y (ex: 1 pour PC2, 3 pour PC4).
    """

    # S√©lectionner les colonnes pour X et Y
    df_selected = df[variables_pc_x + variables_pc_y]

    # Imputation des valeurs manquantes par la moyenne
    imputer = SimpleImputer(strategy='mean')
    df_imputed = imputer.fit_transform(df_selected)

    # Standardisation des donn√©es
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_imputed)

    # V√©rifier combien de composantes peuvent √™tre extraites
    max_components = min(df_scaled.shape)  # min(n_samples, n_features)

    # V√©rifier que les indices de composantes √† afficher sont valides
    if pc_x >= max_components or pc_y >= max_components:
        raise ValueError(f"Impossible d'afficher PC{pc_x+1} et PC{pc_y+1}. "
                         f"Le maximum possible est PC{max_components}.")

    # Appliquer la PCA avec seulement le nombre de composantes n√©cessaires
    pca = PCA(n_components=max_components)
    pca_components = pca.fit_transform(df_scaled)

    # Appliquer K-means sur les donn√©es apr√®s PCA
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(pca_components)

    # Affichage de la projection des individus
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(pca_components[:, pc_x], pca_components[:, pc_y], c=clusters, cmap='viridis', alpha=0.7)
    plt.title(f"Projection des individus sur PC{pc_x+1} et PC{pc_y+1} avec K-means")
    plt.xlabel(f'PC{pc_x+1} (bas√© sur {", ".join(variables_pc_x)})')
    plt.ylabel(f'PC{pc_y+1} (bas√© sur {", ".join(variables_pc_y)})')
    plt.colorbar(scatter, label="Cluster")
    plt.grid(True)
    plt.show()

def plot_pca_individuals(df, plot_correlation_circle, plot_scree_plot, color_col=None):
    """
    Effectue une ACP et affiche le nuage de points des individus selon F1 et F2.
    Agr√®ge √©galement les variables corr√©l√©es fortement avec F1 et F2 en nouvelles variables synth√©tiques.
    
    Param√®tres :
    - df : DataFrame contenant les donn√©es.
    - plot_correlation_circle : Fonction pour afficher le cercle de corr√©lation.
    - plot_scree_plot : Fonction pour afficher le graphique du coude.
    - color_col : Nom de la colonne pour coloriser les points (optionnel).
    """
    # Remplacement des NaN par la m√©diane de chaque colonne
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    imputer = SimpleImputer(strategy='median', missing_values=np.nan)
    df_imputed = pd.DataFrame(imputer.fit_transform(df[numeric_cols]), columns=numeric_cols, index=df.index)
    
    # Standardisation des donn√©es
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_imputed)
    
    # Application de l'ACP
    pca = PCA()
    pca_result = pca.fit_transform(df_scaled)
    
    # Calcul des composantes principales
    pcs = pd.DataFrame(pca.components_.T, index=numeric_cols, 
                        columns=[f'PC{i+1}' for i in range(len(pca.components_))])
    
    # Identifier les variables fortement corr√©l√©es avec F1 et F2
    threshold = 0.5  # Seuil de corr√©lation significative
    f1_vars = pcs.loc[abs(pcs['PC1']) > threshold, 'PC1'].index.tolist()
    f2_vars = pcs.loc[abs(pcs['PC2']) > threshold, 'PC2'].index.tolist()
    
    # Cr√©ation des variables synth√©tiques
    df['F1_synth'] = df_imputed[f1_vars].mean(axis=1) if f1_vars else np.zeros(len(df))
    df['F2_synth'] = df_imputed[f2_vars].mean(axis=1) if f2_vars else np.zeros(len(df))
    
    # Affichage du cercle de corr√©lation
    plot_correlation_circle(pca, numeric_cols)
    
    # Affichage du graphique du coude
    plot_scree_plot(pca)
    
    # Nuage de points des individus
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], 
                           c=df[color_col] if color_col and color_col in df.columns else 'blue', 
                           cmap='viridis', alpha=0.7)
    plt.xlabel("F1")
    plt.ylabel("F2")
    plt.title("Nuage de points des individus (F1 vs F2)")
    if color_col and color_col in df.columns:
        plt.colorbar(scatter, label=color_col)
    plt.grid()
    plt.show()
    
    return df[['F1_synth', 'F2_synth']]

def get_normal_columns(df, columns=None, alpha=0.05, max_sample_size=5000):
    """
    Identifie les colonnes normalement distribu√©es parmi un sous-ensemble de colonnes sp√©cifi√©es.

    :param df: DataFrame pandas
    :param columns: Liste des colonnes √† tester
    :param alpha: Seuil de p-value pour accepter la normalit√©
    :param max_sample_size: Taille maximale de l'√©chantillon pour √©viter les avertissements sur la taille de l'√©chantillon
    :return: Liste des colonnes normalement distribu√©es
    """
    normal_columns = []

    # Si aucune liste de colonnes n'est fournie, prendre toutes les colonnes num√©riques
    if columns is None:
        columns = df.select_dtypes(include=['number']).columns.tolist()

    for col in columns:
        # Filtrer les valeurs num√©riques (en cas de non num√©riques dans la colonne)
        data = df[col].dropna()

        # V√©rifier que la colonne contient des donn√©es num√©riques
        if data.empty or not pd.api.types.is_numeric_dtype(data):
            continue

        # Limiter la taille de l'√©chantillon pour √©viter le warning
        sample = data.sample(n=min(len(data), max_sample_size), random_state=42)

        # Test de Shapiro-Wilk pour la normalit√©
        stat, p_value = shapiro(sample)
        
        # Si p > alpha, on consid√®re que la colonne suit une distribution normale
        if p_value > alpha:
            normal_columns.append(col)

    return normal_columns

def analyze_variables(df, target_col, features, max_sample_size=5000):
    """
    Analyse plusieurs variables quantitatives par rapport √† une variable cat√©gorielle.
    
    :param df: DataFrame pandas
    :param target_col: Variable cat√©gorielle (ex: 'nutrition_grade_fr')
    :param features: Liste des variables quantitatives √† analyser
    :param max_sample_size: Taille max pour le test de Shapiro-Wilk
    """
    # V√©rifier que les colonnes existent
    missing_cols = [col for col in [target_col] + features if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Les colonnes suivantes sont absentes du DataFrame : {missing_cols}")

    # Boucle sur les variables s√©lectionn√©es
    for feature_col in features:
        print(f"\nüîπ Analyse pour {feature_col} :")

        # Suppression des NaN
        df_clean = df[[target_col, feature_col]].dropna()

        # V√©rification si assez de donn√©es
        if df_clean.shape[0] < 10:
            print("‚ö†Ô∏è Trop peu de donn√©es pour analyser cette variable.")
            continue

        # Test de normalit√© (Shapiro-Wilk avec sous-√©chantillonnage)
        stat, p_shapiro = test_shapiro_safe(df_clean[feature_col], max_sample_size)
        normal = p_shapiro > 0.05
        print(f"  - Test de Shapiro-Wilk : p-value = {p_shapiro:.5f} {'‚úÖ Normal' if normal else '‚ùå Non normal'}")

        # Test de Kolmogorov-Smirnov (optionnel)
        stat_ks, p_ks = kstest(df_clean[feature_col], 'norm')
        print(f"  - Test de Kolmogorov-Smirnov : p-value = {p_ks:.5f}")

        # Regroupement par cat√©gorie de la variable cible
        groups = [group[feature_col].values for _, group in df_clean.groupby(target_col)]

        # Test d'homog√©n√©it√© des variances (Levene)
        if len(groups) > 1:
            stat, p_levene = levene(*groups)
            homogene = p_levene > 0.05
            print(f"  - Test de Levene (homog√©n√©it√©) : p-value = {p_levene:.5f} {'‚úÖ Homog√®ne' if homogene else '‚ùå Non homog√®ne'}")
        else:
            print("‚ö†Ô∏è Impossible de tester Levene (une seule cat√©gorie pr√©sente).")
            homogene = False

        # S√©lection du test statistique
        if normal and homogene:
            stat, p = f_oneway(*groups)
            test_type = "ANOVA"
        else:
            stat, p = kruskal(*groups)
            test_type = "Kruskal-Wallis"

        # R√©sultat du test statistique
        print(f"  - {test_type} : p-value = {p:.5f} {'‚ö†Ô∏è Diff√©rence significative !' if p < 0.05 else '‚úÖ Aucune diff√©rence significative.'}")

def plot_nutrition_scatter(df, x_col, y_col, color_col='nutrition-score-fr_100g', cmap='coolwarm'):
    """
    Cr√©e un nuage de points avec une colorisation en fonction du score nutritionnel.
    
    :param df: DataFrame pandas contenant les donn√©es
    :param x_col: Nom de la colonne pour l'axe des X
    :param y_col: Nom de la colonne pour l'axe des Y
    :param color_col: Nom de la colonne pour la colorisation (par d√©faut, 'nutrition-score-fr_100g')
    :param cmap: Colormap utilis√©e pour la colorisation des points (par d√©faut, 'coolwarm')
    """
    
    # V√©rification que les colonnes existent et sont num√©riques
    for col in [x_col, y_col, color_col]:
        if col not in df.columns:
            raise ValueError(f"La colonne '{col}' n'existe pas dans le DataFrame.")
        if not np.issubdtype(df[col].dtype, np.number):
            raise ValueError(f"La colonne '{col}' doit √™tre num√©rique.")
    
    # Suppression des valeurs NaN pour √©viter les erreurs
    df_filtered = df[[x_col, y_col, color_col]].dropna()
    
    # Cr√©ation du nuage de points avec seaborn
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(
        df_filtered[x_col], df_filtered[y_col], 
        c=df_filtered[color_col], cmap=cmap, edgecolors='k', alpha=0.7
    )
    
    # Ajout de la barre de couleur
    cbar = plt.colorbar(scatter)
    cbar.set_label(color_col)
    
    # Ajout des titres et labels
    plt.xlabel(x_col)
    plt.ylabel(y_col)
    plt.title(f'Nuage de points coloris√© par {color_col}')
    
    # Affichage du graphique
    plt.show()

def remove_duplicates(df, id_column):
    """
    Supprime les doublons en se basant uniquement sur un identifiant unique.

    Parameters:
    df (pd.DataFrame): Le DataFrame √† nettoyer.
    id_column (str): Nom de la colonne contenant l'identifiant unique.

    Returns:
    pd.DataFrame: Un DataFrame sans doublons.
    """
    if id_column not in df.columns:
        raise ValueError(f"La colonne '{id_column}' n'existe pas dans le DataFrame.")
    
    df_cleaned = df.drop_duplicates(subset=[id_column])

    return df_cleaned

def filter_duplicates(df, columns=None):
    """
    Filtre le DataFrame pour afficher uniquement les lignes dupliqu√©es sur une ou plusieurs colonnes.

    Parameters:
    df (pd.DataFrame): Le DataFrame √† analyser.
    columns (list or str, optional): Nom d'une colonne ou liste de colonnes √† v√©rifier. 
                                      Si None, v√©rifie tous les doublons sur toutes les colonnes.

    Returns:
    pd.DataFrame: DataFrame contenant uniquement les lignes en double.
    """
    if columns:
        if isinstance(columns, str):
            columns = [columns]  # Convertir en liste si une seule colonne est donn√©e
        duplicates = df[df.duplicated(subset=columns, keep=False)]  # Garder toutes les occurrences en doublon
    else:
        duplicates = df[df.duplicated(keep=False)]  # V√©rifier les doublons sur toutes les colonnes

    return duplicates

def calculate_mode(df, columns=None):
    """
    Calcule le mode d'un DataFrame ou des colonnes sp√©cifiques renseign√©es.

    Parameters:
    df (pd.DataFrame): Le DataFrame √† analyser.
    columns (list, optional): Liste des colonnes sur lesquelles calculer le mode. Si None, calcule le mode sur toutes les colonnes.

    Returns:
    pd.Series: Mode des colonnes sp√©cifi√©es.
    """
    if columns:
        return df[columns].mode()
    else:
        return df.mode()

def calculate_mean(df, columns=None):
    """
    Calcule la moyenne d'un DataFrame ou des colonnes sp√©cifiques renseign√©es.

    Parameters:
    df (pd.DataFrame): Le DataFrame √† analyser.
    columns (list, optional): Liste des colonnes sur lesquelles calculer la moyenne. Si None, calcule la moyenne sur toutes les colonnes.

    Returns:
    pd.Series: Moyenne des colonnes sp√©cifi√©es.
    """
    if columns:
        return df[columns].mean()
    else:
        return df.mean()

def calculate_median(df, columns=None):
    """
    Calcule la m√©diane d'un DataFrame ou des colonnes sp√©cifiques renseign√©es.

    Parameters:
    df (pd.DataFrame): Le DataFrame √† analyser.
    columns (list, optional): Liste des colonnes sur lesquelles calculer la m√©diane. Si None, calcule la m√©diane sur toutes les colonnes.

    Returns:
    pd.Series: M√©diane des colonnes sp√©cifi√©es.
    """
    if columns:
        return df[columns].median()
    else:
        return df.median()


def replace_nan_with_stat(df, columns, stat='mode'):
    """
    Remplace les NaN dans les colonnes sp√©cifi√©es par une statistique choisie (mode, mean ou median).

    :param df: DataFrame pandas
    :param columns: Liste des colonnes √† traiter
    :param stat: Statistique utilis√©e pour remplacer les NaN ('mode', 'mean', 'median')
    :return: DataFrame avec valeurs remplac√©es
    """
    df = df.copy()  # √âviter les modifications de l'original

    for col in columns:
        if stat == 'mode':
            value = df[col].mode().dropna().iloc[0] if not df[col].mode().dropna().empty else np.nan
        elif stat == 'mean':
            value = df[col].mean(skipna=True)
        elif stat == 'median':
            value = df[col].median(skipna=True)
        else:
            raise ValueError("Stat must be 'mode', 'mean', or 'median'")

        # Remplacer NaN avec la valeur calcul√©e
        df[col] = df[col].fillna(value)

    return df


def display_boxplot_with_stats(dataframe, column_name):
    """
    Affiche un graphique box plot pour une colonne donn√©e d'un DataFrame
    et ajoute des lignes pour la moyenne, la m√©diane et l'√©cart-type.
    
    Parameters:
        dataframe (pd.DataFrame): Le DataFrame contenant les donn√©es.
        column_name (str): Le nom de la colonne √† afficher dans le boxplot.
    """
    # Calcul des statistiques
    mean = dataframe[column_name].mean()
    median = dataframe[column_name].median()
    std = dataframe[column_name].std()

    # Box plot
    dataframe.boxplot(column=column_name)

    # Ajouter la ligne de la moyenne
    plt.axhline(y=mean, color='r', linestyle='--', label='Moyenne')

    # Ajouter la ligne de la m√©diane
    plt.axhline(y=median, color='b', linestyle='-', label='M√©diane')

    # Ajouter les lignes de l'√©cart-type
    plt.axhline(y=mean + std, color='g', linestyle=':', label='√âcart-type (+)')
    plt.axhline(y=mean - std, color='g', linestyle=':', label='√âcart-type (-)')

    # Ajouter des annotations textuelles avec des positions ajust√©es
    # Moyenne (au-dessus)
    plt.annotate(f'Moyenne: {mean:.2f}', 
                 xy=(0.5, mean + 0.05), xycoords='data',  # L√©g√®rement au-dessus de la ligne
                 bbox=dict(boxstyle="round,pad=0.3", edgecolor='red', facecolor='white'),
                 color='red', fontsize=10, ha='center')

    # M√©diane (au-dessous)
    plt.annotate(f'M√©diane: {median:.2f}', 
                 xy=(0.85, median), xycoords='data',  # L√©g√®rement en dessous de la ligne
                 bbox=dict(boxstyle="round,pad=0.3", edgecolor='blue', facecolor='white'),
                 color='blue', fontsize=10, ha='center')

    # √âcart-type (√† droite)
    plt.annotate(f'√âcart-type: {std:.2f}', 
                 xy=(1.15, mean + std), xycoords='data',  # √Ä droite de la ligne
                 bbox=dict(boxstyle="round,pad=0.3", edgecolor='green', facecolor='white'),
                 color='green', fontsize=10, ha='left')

    # Titre et labels
    plt.title(f'Box Plot {column_name}')
    plt.ylabel(column_name)

    # Ajouter une l√©gende
    plt.legend()

    # Ajuster la mise en page
    plt.tight_layout()

    # Afficher le graphique
    plt.show()


def detect_outliers(df, columns, method="IQR", plausibility_check=True):
    """
    D√©tecte les valeurs aberrantes dans les colonnes sp√©cifi√©es du DataFrame.
    
    Args:
    - df (pd.DataFrame): Le DataFrame contenant les donn√©es.
    - columns (list): Liste des colonnes √† analyser.
    - method (str): M√©thode √† utiliser ("IQR" ou "Z-score").
    - plausibility_check (bool): Si True, applique des seuils de plausibilit√© d√©finis.

    Returns:
    - pd.DataFrame: Un DataFrame indiquant les valeurs aberrantes (True/False).
    """
    outliers = pd.DataFrame(False, index=df.index, columns=columns)

    # Seuils de plausibilit√©
    plausibility_thresholds = {
        'omega-3-fat_100g': (0, 30),
        'omega-6-fat_100g': (0, 50),
        'iron_100g': (0, 15),
        'calcium_100g': (0, 1500),
        'energy-from-fat_100g': (0, 900)
    }

    for col in columns:
        if df[col].dtype in [np.float64, np.int64]:  
            col_data = df[col].dropna()

            # D√©tection par IQR
            if method == "IQR":
                Q1 = col_data.quantile(0.25)
                Q3 = col_data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers[col] = (df[col] < lower_bound) | (df[col] > upper_bound)

            # D√©tection par Z-score (calcul manuel)
            elif method == "Z-score":
                mean = col_data.mean()
                std = col_data.std()
                z_scores = (df[col] - mean) / std
                outliers[col] = np.abs(z_scores) > 3

            # V√©rification de plausibilit√©
            if plausibility_check and col in plausibility_thresholds:
                min_val, max_val = plausibility_thresholds[col]
                outliers[col] |= (df[col] < min_val) | (df[col] > max_val)

    return outliers

def replace_outliers(df, outliers_detected, columns, strategy="median", replace_with_nan=False):
    """
    Remplace les outliers d√©tect√©s dans un DataFrame par une valeur sp√©cifique ou NaN.
    
    Param√®tres :
    - df : pd.DataFrame -> DataFrame contenant les donn√©es.
    - outliers_detected : dict -> Dictionnaire des outliers avec colonnes et indices.
    - columns : list -> Liste des colonnes √† traiter.
    - strategy : str -> Strat√©gie de remplacement ('median', 'mean', 'mode').
    - replace_with_nan : bool -> Si True, remplace les outliers par NaN au lieu d'une statistique.
    
    Retourne :
    - pd.DataFrame -> DataFrame avec les valeurs aberrantes remplac√©es.
    """
    df_cleaned = df.copy()

    for col in columns:
        if col in df_cleaned.columns:
            if replace_with_nan:
                replacement_value = np.nan
            else:
                if strategy == "median":
                    replacement_value = df_cleaned[col].median()
                elif strategy == "mean":
                    replacement_value = df_cleaned[col].mean()
                elif strategy == "mode":
                    replacement_value = df_cleaned[col].mode()[0] if not df_cleaned[col].mode().empty else np.nan
                else:
                    raise ValueError("La strat√©gie doit √™tre 'median', 'mean' ou 'mode'.")
            
            print(f"üîπ {col} - Valeur utilis√©e pour remplacer les outliers : {replacement_value}")
            
            # Remplacement des outliers d√©tect√©s
            df_cleaned.loc[outliers_detected[col], col] = replacement_value

            # V√©rification : Remplir les NaN restants avec la m√™me strat√©gie si replace_with_nan est False
            if not replace_with_nan:
                df_cleaned[col] = df_cleaned[col].fillna(replacement_value)
    
    return df_cleaned

def plot_histogram(df, column):
    """
    Affiche un histogramme pour une variable donn√©e d'un DataFrame avec estimation KDE.
    Trace √©galement les seuils des outliers selon la m√©thode interquartile (IQR).
    
    :param df: DataFrame pandas
    :param column: Nom de la colonne √† visualiser
    """
    # Suppression des valeurs NaN
    data = df[column].dropna()
    
    # Calcul des quartiles et de l'IQR
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1
    
    # Seuils des outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cr√©ation du graphique
    plt.figure(figsize=(8, 5))
    sns.histplot(data, bins=30, kde=True, color="royalblue")
    
    # Ajout des lignes verticales pour les seuils des outliers
    plt.axvline(lower_bound, color='red', linestyle='dashed', linewidth=2, label=f"Seuil bas ({lower_bound:.2f})")
    plt.axvline(upper_bound, color='red', linestyle='dashed', linewidth=2, label=f"Seuil haut ({upper_bound:.2f})")
    
    # Personnalisation
    plt.title(f'Histogramme de {column} avec d√©tection des outliers (IQR)')
    plt.xlabel(column)
    plt.ylabel('Fr√©quence')
    plt.legend()
    
    # Affichage du graphique
    plt.show()

def plot_scatter(df, column_x, column_y):
    """
    Affiche un diagramme de dispersion pour deux variables d'un DataFrame.
    :param df: DataFrame pandas
    :param column_x: Nom de la colonne pour l'axe X
    :param column_y: Nom de la colonne pour l'axe Y
    """
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=df[column_x], y=df[column_y])
    plt.title(f'Diagramme de dispersion entre {column_x} et {column_y}')
    plt.xlabel(column_x)
    plt.ylabel(column_y)
    plt.show()

def plot_heatmap(df, columns=None, method='spearman', figsize=(8,6), cmap="coolwarm", annot=True):
    """
    G√©n√®re une heatmap de corr√©lation pour les colonnes s√©lectionn√©es d'un DataFrame.

    :param df: pd.DataFrame - Le DataFrame contenant les donn√©es.
    :param columns: list - Liste des colonnes √† inclure dans la heatmap (par d√©faut, toutes les colonnes num√©riques).
    :param method: str - M√©thode de corr√©lation ('spearman', 'pearson', 'kendall').
    :param figsize: tuple - Taille de la figure (par d√©faut : (8,6)).
    :param cmap: str - Palette de couleurs pour la heatmap.
    :param annot: bool - Afficher ou non les coefficients dans les cases.
    """
    
    # S√©lection des colonnes √† analyser
    if columns is None:
        df_selected = df.select_dtypes(include=['number'])  # S√©lectionne seulement les colonnes num√©riques
    else:
        df_selected = df[columns]
    
    # Calcul de la matrice de corr√©lation
    corr_matrix = df_selected.corr(method=method)
    
    # Cr√©ation de la heatmap avec Seaborn
    plt.figure(figsize=figsize)
    sns.heatmap(corr_matrix, annot=annot, cmap=cmap, fmt=".2f", linewidths=0.5, square=True, cbar=True)

    # Ajout d'un titre
    plt.title(f"Heatmap des corr√©lations ({method.capitalize()})")
    plt.show()

def test_spearman(df, col1, col2):
    """
    Effectue le test de Spearman entre deux colonnes et affiche le coefficient de corr√©lation et la p-value.
    :param df: DataFrame contenant les donn√©es
    :param col1: Nom de la premi√®re colonne
    :param col2: Nom de la deuxi√®me colonne
    """
    coef, p_value = spearmanr(df[col1], df[col2], nan_policy='omit')

    print(f"Test de Spearman entre {col1} et {col2}:")
    print(f"üìä Coefficient de corr√©lation : {coef:.3f}")
    print(f"üìâ P-value : {p_value:.5f}")
    
    if p_value < 0.05:
        print("‚úÖ La corr√©lation est significative (p < 0.05).")
    else:
        print("‚ùå La corr√©lation n'est pas significative (p >= 0.05).")
    print("-" * 50)

def plot_elbow_curve(data_scaled):
    """
    Applique une ACP sur les donn√©es standardis√©es et trace la courbe du coude.

    Param√®tres :
    - data_scaled : array numpy, donn√©es pr√©trait√©es et standardis√©es

    Retour :
    - pca : objet PCA ajust√©
    - explained_variance : array, variance expliqu√©e par chaque composante
    """
    # Appliquer l'ACP
    pca = PCA()
    pca.fit(data_scaled)

    # Calculer les valeurs propres (variance expliqu√©e)
    explained_variance = pca.explained_variance_ratio_

    # Tracer la courbe du coude
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), 
             marker='o', linestyle='--', color='b')
    plt.xlabel("Nombre de composantes principales")
    plt.ylabel("Variance expliqu√©e cumul√©e")
    plt.title("M√©thode du coude pour choisir le nombre optimal de composantes")
    plt.grid()
    plt.show()

    return pca, explained_variance

def plot_elbow_curve2(data_scaled):
    """
    Applique une ACP sur les donn√©es standardis√©es et trace la courbe du coude.

    Param√®tres :
    - data_scaled : array numpy, donn√©es pr√©trait√©es et standardis√©es

    Retour :
    - pca : objet PCA ajust√©
    """
    # Appliquer l'ACP
    pca = PCA()
    pca.fit(data_scaled)

    # Calculer les valeurs propres (variance expliqu√©e)
    explained_variance = pca.explained_variance_ratio_

    # Tracer la courbe du coude
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), 
             marker='o', linestyle='--', color='b')
    plt.xlabel("Nombre de composantes principales")
    plt.ylabel("Variance expliqu√©e cumul√©e")
    plt.title("M√©thode du coude pour choisir le nombre optimal de composantes")
    plt.grid()
    plt.show()

    # Retourner uniquement l'objet PCA
    return pca


def plot_correlation_circle(pca, columns_for_pca):
    """
    Tracer le cercle des corr√©lations (ACP) avec les composantes principales.

    Parameters:
    pca : mod√®le PCA
        Mod√®le PCA entra√Æn√© qui contient les informations des composantes principales.
    columns_for_pca : list
        Liste des noms des variables utilis√©es pour l'ACP.
    """
    # Calcul des coordonn√©es des variables dans l'espace des composantes principales
    components = pca.components_

    # Cr√©er la figure et les axes
    fig, ax = plt.subplots(figsize=(7, 7))

    # Ajouter le cercle unit√©
    circle = patches.Circle((0, 0), radius=1, edgecolor='black', facecolor='none', linestyle='dashed')
    ax.add_patch(circle)

    # Tracer les fl√®ches pour chaque variable
    for i, column in enumerate(columns_for_pca):
        plt.arrow(0, 0, components[0, i], components[1, i], 
                  head_width=0.05, head_length=0.05, color='r')
        plt.text(components[0, i], components[1, i], column, fontsize=12)

    # Limites et lignes de r√©f√©rence
    plt.xlim(-1.1, 1.1)
    plt.ylim(-1.1, 1.1)
    plt.axhline(0, color='black', linewidth=0.5)
    plt.axvline(0, color='black', linewidth=0.5)

    # Titre et affichage
    plt.title("Cercle des corr√©lations (ACP)")
    plt.grid()
    plt.show()

def plot_pca_projection(pca, data_scaled, df_clean, column_for_color):
    """
    Trace la projection des individus dans l'espace des 2 premi√®res composantes principales (PCA),
    avec colorisation bas√©e sur une variable sp√©cifique.

    Param√®tres :
    - pca : mod√®le PCA ajust√©
    - data_scaled : array numpy, donn√©es standardis√©es
    - df_clean : DataFrame contenant les donn√©es nettoy√©es
    - column_for_color : string, nom de la colonne √† utiliser pour la colorisation
    """
    # Extraire les coordonn√©es des individus dans l'espace des composantes principales
    df_pca = pca.transform(data_scaled)

    # Ajouter la colonne pour la colorisation
    df_clean[column_for_color] = df_clean[column_for_color]

    # Tracer la projection
    plt.figure(figsize=(8,6))
    sns.scatterplot(x=df_pca[:, 0], y=df_pca[:, 1], hue=df_clean[column_for_color], palette="Set1", alpha=0.7)
    plt.xlabel("Composante 1")
    plt.ylabel("Composante 2")
    plt.title("Projection des individus (coloris√© par " + column_for_color + ")")
    plt.legend(title=column_for_color,  loc='upper right')
    plt.grid()
    plt.show()

def impute_missing_values(df, columns=None, estimator=BayesianRidge(), max_iter=10, random_state=42):
    """
    Remplace les valeurs NaN dans certaines colonnes ou dans tout le DataFrame avec IterativeImputer.
    
    :param df: DataFrame pandas contenant des valeurs NaN
    :param columns: Liste des colonnes √† imputer (par d√©faut, toutes les colonnes num√©riques)
    :param estimator: Mod√®le utilis√© pour l'imputation (par d√©faut, BayesianRidge)
    :param max_iter: Nombre maximal d'it√©rations pour l'imputation
    :param random_state: Graine al√©atoire pour la reproductibilit√©
    :return: Nouveau DataFrame avec les valeurs NaN remplac√©es
    """
    df_imputed = df.copy()  # Cr√©ation d'une copie du DataFrame original
    
    # S√©lection des colonnes concern√©es
    if columns is None:
        columns = df_imputed.select_dtypes(include=[np.number]).columns  # Colonnes num√©riques uniquement
    
    # V√©rification qu'il y a bien des colonnes s√©lectionn√©es
    if len(columns) == 0:
        raise ValueError("Aucune colonne valide pour l'imputation.")
    
    # Initialisation de l'IterativeImputer
    imputer = IterativeImputer(estimator=estimator, max_iter=max_iter, random_state=random_state)
    
    # Imputation sur les colonnes s√©lectionn√©es
    df_imputed[columns] = imputer.fit_transform(df_imputed[columns])
    
    return df_imputed